# Reflection on Defensive Design: Data Biases in AI Hiring Tools

Artificial intelligence is becoming an essential part of our lives whether it’s voice assistants, driverless cars, or personalized advertisements. While these technologies appear helpful and interesting, artificial intelligence is also being used to make important decisions such as whether a person is authorized for a loan, accepted as a rental applicant, or gets a job. 

Automated employment decision tool is a commonly used computational process derived from ML, statistical modeling, data analytics, and/or AI to achieve a simplified output like a rating or recommendation. These tools include game-based tests, resume review tools, and automated personality tests to assist or replace decision-making. 
Researchers have highlighted employers granting interviews at differing rates to job applicants with identical resumes but with names reflecting different racial groups. Some employers also review prospective employees’ credit histories in ways that can harm minority groups even though there is no established link between job performance and credit history. 

Bias can be introduced to data through how it is collected and interpreted. Models could be trained on data containing biased human decisions that reflect societal or historical inequities. AI has the potential to help humans make decisions but only if employers mitigate bias and establish measures to ensure fairness in AI hiring/recruiting tools. In identifying bias, individuals can rethink the standards used to determine when decisions are fair or reflect bias. 

# Industry Examples

Since 2014, Amazon had been developing an AI recruiting engine to review resumes and give job candidates scores ranging from one to five stars. By 2015, Amazon realized the system did not like women. The model was trained using resumes submitted to the company over the previous ten years with the majority coming from men reflecting the male dominance of the tech industry. As a result, the algorithm penalized resumes with words like "women's" such as women's college and favored candidates using verbs commonly found on male engineers' resumes such as "captured" and "executed." By 2017, Amazon shut down the project and now uses another version.

The problem doesn’t end with Amazon. According to Harvard Business School, automated resume-scanning software contributes to biased hiring systems. These programs are used by 75% of US employers and 99% of Fortune 500 companies. Some systems automatically reject candidates with gaps of longer than six months in their employment history which could be due to an illness, pregnancy, or a recession. More specific examples include retail clerk positions requiring floor-buffing and hospitals requiring programming experience. Overall, automated resume scanning software evaluates applicants based on subjective criteria such as gender and preferred names which can discount the candidate’s true potential. 

# Where does this bias come from? 

Data is inherently biased because it doesn’t represent the whole picture. For example, imagine you are trying to collect heart rate data to get an average resting heart rate. If you collect data in a hospital, the data will then not include people who don’t have health insurance or students who are far away from home and are not getting yearly physicals. Machine learning software could be trained on datasets that underrepresent a particular ethnic or social group and consider algorithmic bias. However, according to a NIST report, there is more to AI bias than biased data. Bias not only appears in AI algorithms and training data but in the societal context AI systems are used, so we must consider human and systemic biases. Human biases occur when individuals attempt to fill in missing information using stereotypes or generalities such as a person’s race influencing how likely authorities would consider the person to be a crime suspect. Systemic biases are a result of institutions disadvantaging certain ethnic or social groups such as discriminating against individuals based on race, religion, age, or gender. To mitigate biases in AI, it is crucial to address the model and training data as well as human and systemic biases. 

# Potential Solutions 

1. Be informed of algorithmic biases. Companies must be aware of contexts where AI can help correct bias and those where there is high risk for AI to exacerbate bias. When deploying AI, it is important to anticipate domains potentially prone to unfair bias. 
2. Establish transparency and standard testing practices. Employers must establish processes to test for and mitigate bias in AI systems. Third parties should be used to audit data and models. Companies should be transparent about processes to allow observers to understand the steps taken to promote fairness. 
3. Address human and systemic biases. Engage in fact-based conversations about potential biases in human decisions.  
4. Filter data and monitor models. Filter training data using algorithms to ensure training data sets are unbiased. Make sure data is representative and large enough to prevent common biases. The model should be monitored over time. 
5. Invest in diversity in AI. Invest in diversifying the AI field since it does not encompass society’s diversity. It could then better review issues of unfair bias and engage communities affected by bias. This requires investing in AI education as well as opportunities. 
6. Bias research. Invest more in bias research with more data available for research and adopt a multidisciplinary approach.

# NYC Automated Employment Decision Tool Law

New York City is implementing an automated employment decision tool law which will be enforced starting January 1st, 2023. Employers will need to ensure they are in compliance with the AI bias law which is designed to protect employees from unlawful bias during hiring processes when AI tools are used. 

    Fairness: Avoid discriminating against candidates based on race, religion, age, and gender
    Transparency/Trustworthiness: Must disclose bias audit summary and qualifications AI tool is searching for 
    Accountability: Yearly bias audit performed with penalties if utilizing biased system  
    Responsibility: 10 days notice to candidates of qualifications and characteristics AI is looking for and allow applicants to request alternative selection process or accommodations

According to Forbes, almost all Fortune 500 companies use AI software to assist in recruiting and other employment decisions. Because of NYC’s AI bias law, employers need to understand the automated recruiting tools they’re using which may require discussions with outside vendors that employ such tools on behalf of the employer. Additionally, they need to ensure the AI is not solely recommending candidates for interviews because they resemble the company’s current employees which may not be diverse. AI tends to penalize job candidates with gaps in their resume leading to bias against older women who may have taken time off for childcare or college students who took a gap year to travel or work. 

Companies are now required to conduct an annual bias audit, provide disclosure to candidates, notify the applicants, and offer an alternative selection procedure or accommodation. The annual bias audit must be an impartial evaluation by an independent auditor. The company must also publish a summary of bias audit results on their website, the type of data collected by the tool, and the source of data. Employers are also required to notify candidates at least 10 days before using the tool to assess employment, job qualifications, and allow the candidate to request an alternative selection procedure or accommodation. Some employers use automated games that are unfair to people with disabilities who may benefit from reasonable accommodation such as larger text size or change in screen display. If companies do not comply, they will suffer penalties. 

In conclusion, it is crucial to look beyond machine learning processes and training data to the societal factors that influence how technology is developed. Bias is inherent not only in algorithms but the societal context in which artificial intelligence systems are used. A more diverse AI community would be better equipped to anticipate and review bias as well as engage affected communities. 
